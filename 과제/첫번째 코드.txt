/////////////////////////////////////////
첫번째 코드


import numpy as np
import torch
import random
from agents.rl.submission import agent as rl_agent
from env.chooseenv import make
from tabulate import tabulate
import argparse
import os

actions_map = {0: [-100, -30], 1: [-100, -18], 2: [-100, -6], 3: [-100, 6], 4: [-100, 18], 5: [-100, 30],
               6: [-40, -30], 7: [-40, -18], 8: [-40, -6], 9: [-40, 6], 10: [-40, 18], 11: [-40, 30],
               12: [20, -30], 13: [20, -18], 14: [20, -6], 15: [20, 6], 16: [20, 18], 17: [20, 30],
               18: [80, -30], 19: [80, -18], 20: [80, -6], 21: [80, 6], 22: [80, 18], 23: [80, 30],
               24: [140, -30], 25: [140, -18], 26: [140, -6], 27: [140, 6], 28: [140, 18], 29: [140, 30],
               30: [200, -30], 31: [200, -18], 32: [200, -6], 33: [200, 6], 34: [200, 18], 35: [200, 30]}

RENDER = True

def get_join_actions(state, algo_list):
    joint_actions = []
    for agent_idx in range(len(algo_list)):
        if algo_list[agent_idx] == 'random':
            driving_force = random.uniform(-100, 200)
            turning_angle = random.uniform(-30, 30)
            joint_actions.append([[driving_force], [turning_angle]])
        elif algo_list[agent_idx] == 'rl':
            obs = state[agent_idx]['obs'].flatten()
            actions_raw = rl_agent.choose_action(obs)
            actions = actions_map[actions_raw]
            joint_actions.append([[actions[0]], [actions[1]]])
    return joint_actions

# -----------------------------
# 환경 제어용: 특정 구역에 머무르게 강제
def restrict_zone_env_one_agent(state, joint_action, agent_idx=1):
    pos = state[agent_idx]['position']
    if 50 <= pos[0] <= 70 and 20 <= pos[1] <= 40:
        joint_action[agent_idx][0] = [0]
        joint_action[agent_idx][1] = [0]
    return joint_action

# 보상 유도용: 특정 구역에 머물면 보상 추가
def reward_zone_bonus_one_agent(state, reward, agent_idx=1):
    pos = state[agent_idx]['position']
    if 50 <= pos[0] <= 70 and 20 <= pos[1] <= 40:
        reward[agent_idx] += 1
    return reward
# -----------------------------

def run_game(env, algo_list, episode, shuffle_map, map_num,
             env_control=False, reward_bonus=False, verbose=False):
    total_reward = np.zeros(2)
    num_win = np.zeros(3)
    total_steps = []

    for i in range(1, int(episode)+1):
        episode_reward = np.zeros(2)
        state = env.reset(shuffle_map)
        if RENDER:
            env.env_core.render()
        step = 0

        while True:
            joint_action = get_join_actions(state, algo_list)

            # 1️⃣ 환경 제어 적용
            if env_control:
                joint_action = restrict_zone_env_one_agent(state, joint_action, agent_idx=1)

            next_state, reward, done, _, info = env.step(joint_action)

            # 2️⃣ 보상 유도 적용
            if reward_bonus:
                reward = reward_zone_bonus_one_agent(next_state, reward, agent_idx=1)

            reward = np.array(reward)
            episode_reward += reward

            if RENDER:
                env.env_core.render()

            step += 1
            if done:
                if reward[0] != reward[1]:
                    if reward[0] == 100:
                        num_win[0] += 1
                    elif reward[1] == 100:
                        num_win[1] += 1
                        total_steps.append(step)
                else:
                    num_win[2] += 1

                if not verbose:
                    print('.', end='')
                    if i % 100 == 0 or i == episode:
                        print()
                break
            state = next_state

        total_reward += episode_reward

    total_reward /= episode
    average_steps = np.mean(total_steps) if total_steps else 0

    print("total reward:", total_reward)
    print(f'Result in map {map_num} within {episode} episode:')
    header = ['Name', algo_list[0], algo_list[1]]
    data = [['score', np.round(total_reward[0], 2), np.round(total_reward[1], 2)],
            ['win', num_win[0], num_win[1]],
            ['avg_steps', average_steps, '-']]
    print(tabulate(data, headers=header, tablefmt='pretty'))

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--my_ai", default='rl', help='rl/random')
    parser.add_argument("--opponent", default='random', help='rl/random')
    parser.add_argument("--episode", default=20)
    parser.add_argument("--map", default='all', help='1/2/3/4/all')
    parser.add_argument("--env_control", action='store_true', help='Apply environment control to stay in zone')
    parser.add_argument("--reward_bonus", action='store_true', help='Apply reward bonus to stay in zone')
    args = parser.parse_args()

    env_type = "olympics-running"
    game = make(env_type, conf=None, seed=1)

    if args.map != 'all':
        game.specify_a_map(int(args.map))
        shuffle = False
    else:
        shuffle = True

    agent_list = [args.opponent, args.my_ai]

    # ⚠️ 두 옵션 중 하나만 켜서 실행
    run_game(game, algo_list=agent_list, episode=args.episode,
             shuffle_map=shuffle, map_num=args.map,
             env_control=args.env_control, reward_bonus=args.reward_bonus)
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
